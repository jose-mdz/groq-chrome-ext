export const testDocument = `Jump to content\nMain menu\nSearch\nAppearance\nPersonal tools\nToggle the table of contents\nLarge language model\n46 languages\nArticle\nTalk\nTools\nFrom Wikipedia, the free encyclopedia\nSorry to interrupt, but our fundraiser won't last long\nThis Tuesday, we ask you to join the 2% of readers who give. If everyone reading this right now gave just $2.75, we'd hit our goal in a couple of hours. $2.75 is all we ask.\nGIVE $2.75 MAYBE LATER\n December 10: Wikipedia still can't be sold\nWe're sorry we've asked you a few times recently, but it's Tuesday, December 10 – please don't wait until tomorrow to help. We're happy you consult Wikipedia often. If everyone reading this gave $2.75 today, we'd hit our goal in a few hours. Just 2% of our readers donate, so if Wikipedia has given you $2.75 worth of knowledge, please give. Any contribution helps, whether it's $2.75 or $25.\nGive $2.75\n \nGive a different amount\n\nProud host of Wikipedia and its sister sites\n\nMAYBE LATER \nI ALREADY DONATED\nCLOSE \nNot to be confused with Logic learning machine.\nPart of a series on\nMachine learning\nand data mining\n\nshow\nParadigms\n\n\nshow\nProblems\n\n\nshow\nSupervised learning\n(classification • regression)\n\n\nshow\nClustering\n\n\nshow\nDimensionality reduction\n\n\nshow\nStructured prediction\n\n\nshow\nAnomaly detection\n\n\nhide\nArtificial neural network\nAutoencoderDeep learningFeedforward neural networkRecurrent neural network LSTMGRUESNreservoir computingBoltzmann machine RestrictedGANDiffusion modelSOMConvolutional neural network U-NetLeNetAlexNetDeepDreamNeural radiance fieldTransformer VisionMambaSpiking neural networkMemtransistorElectrochemical RAM (ECRAM)\n\n\nshow\nReinforcement learning\n\n\nshow\nLearning with humans\n\n\nshow\nModel diagnostics\n\n\nshow\nMathematical foundations\n\n\nshow\nJournals and conferences\n\n\nshow\nRelated articles\n\nvte\n\nA large language model (LLM) is a type of computational model designed for natural language processing tasks such as language generation. As language models, LLMs acquire these abilities by learning statistical relationships from vast amounts of text during a self-supervised and semi-supervised training process.[1]\n\nThe largest and most capable LLMs are artificial neural networks built with a decoder-only transformer-based architecture, enabling efficient processing and generation of large-scale text data. Modern models can be fine-tuned for specific tasks or guided by prompt engineering.[2] These models acquire predictive power regarding syntax, semantics, and ontologies[3] inherent in human language corpora, but they also inherit inaccuracies and biases present in the data they are trained in.[4]\n\n`;

export const testSelection = `History[edit]\n\nThe training compute of notable large models in FLOPs vs publication date over the period 2010-2024. For overall notable models (top left), frontier models (top right), top language models (bottom left) and top models within leading companies (bottom right). The majority of these models are language models.\n\nThe training compute of notable large AI models in FLOPs vs publication date over the period 2017-2024. The majority of large models are language models or multimodal models with language capacity.\nBefore 2017, there were a few language models that were large as compared to capacities then available. In the 1990s, the IBM alignment models pioneered statistical language modelling. A smoothed n-gram model in 2001 trained on 0.3 billion words achieved state-of-the-art perplexity at the time.[5] In the 2000s, as Internet use became prevalent, some researchers constructed Internet-scale language datasets ("web as corpus"[6]), upon which they trained statistical language models.[7][8] In 2009, in most language processing tasks, statistical language models dominated over symbolic language models, as they can usefully ingest large datasets.[9]\n\nAfter neural networks became dominant in image processing around 2012,[10] they were applied to language modelling as well. Google converted its translation service to Neural Machine Translation in 2016. As it was before transformers, it was done by seq2seq deep LSTM networks.\n\n\nAn illustration of main components of the transformer model from the original paper, where layers were normalized after (instead of before) multiheaded attention\nAt the 2017 NeurIPS conference, Google researchers introduced the transformer architecture in their landmark paper "Attention Is All You Need". This paper's goal was to improve upon 2014 seq2seq technology,[11] and was based mainly on the attention mechanism developed by Bahdanau et al. in 2014.[12] The following year in 2018, BERT was introduced and quickly became "ubiquitous".[13] Though the original transformer has both encoder and decoder blocks, BERT is an encoder-only model. Academic and research usage of BERT began to decline in 2023, following rapid improvements in the abilities of decoder-only models (such as GPT) to solve tasks via prompting.[14]\n\n`;
